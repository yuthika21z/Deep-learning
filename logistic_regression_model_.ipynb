{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdP/eieN8QXnCuLTNcg17j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuthika21z/Deep-learning/blob/main/logistic_regression_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IofFsPxds4QJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    train_ds = h5py.File('./dataset/train_catvnoncat.h5', 'r')\n",
        "    train_set_x = np.array(train_ds['train_set_x'][:])\n",
        "    train_set_y = np.array(train_ds['train_set_y'][:])\n",
        "\n",
        "    test_ds = h5py.File('./dataset/test_catvnoncat.h5', 'r')\n",
        "    test_set_x = np.array(test_ds['test_set_x'][:])\n",
        "    test_set_y = np.array(test_ds['test_set_y'][:])\n",
        "\n",
        "    classes = np.array(test_ds['list_classes'][:])\n",
        "\n",
        "    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
        "    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
        "\n",
        "    return train_set_x, train_set_y, test_set_x, test_set_y, classes"
      ],
      "metadata": {
        "id": "9XYJsv3WtGGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    return s"
      ],
      "metadata": {
        "id": "6upbjQ-qtMTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smWIymhDtPNs",
        "outputId": "037b7dd9-2a2d-49ba-88ed-aacca215667a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid([0, 2]) = [0.5        0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([0.5, 0, 2.0])\n",
        "output = sigmoid(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmhjybJctSJc",
        "outputId": "49de6447-505d-400a-ca3e-b207562b3393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62245933 0.5        0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias) of type float\n",
        "    \"\"\"\n",
        "    w = np.zeros(shape=(dim, 1), dtype=np.float32)\n",
        "    b = 0.0\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "mjhohknLtr3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "\n",
        "assert type(b) == float\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPlE_5lZtvGc",
        "outputId": "0b2e9655-9afc-4381-a3e2-3330d3059823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[0.]\n",
            " [0.]]\n",
            "b = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "\n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # forward propagation (from x to cost)\n",
        "    # compute activation\n",
        "    A = sigmoid(w.T @ X + b)\n",
        "    # compute cost by using np.dot to perform multiplication\n",
        "    cost = np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / -m\n",
        "\n",
        "    # backward propagation (to find grad)\n",
        "    dw = X @ (A - Y).T / m\n",
        "    db = np.sum(A - Y) / m\n",
        "\n",
        "    cost = np.squeeze(np.array(cost))\n",
        "\n",
        "    grads = {'dw': dw, 'db': db}\n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "Et4JRDCGtx1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w =  np.array([[1.], [2]])\n",
        "b = 1.5\n",
        "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
        "Y = np.array([[1, 1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "assert type(grads[\"dw\"]) == np.ndarray\n",
        "assert grads[\"dw\"].shape == (2, 1)\n",
        "assert type(grads[\"db\"]) == np.float64\n",
        "\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPjuM2Bht1R-",
        "outputId": "0aa33a63-e8d1-4b89-bc2f-8ca2671eb78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dw = [[ 0.25071532]\n",
            " [-0.06604096]]\n",
            "db = -0.12500404500439652\n",
            "cost = 0.15900537707692405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    w = copy.deepcopy(w)\n",
        "    b = copy.deepcopy(b)\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # cost and gradient calculation\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "            # Print the cost every 100 training iterations\n",
        "            if print_cost:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "9L0qyOv2t5CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print(\"Costs = \" + str(costs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAVgjN6Xt9PM",
        "outputId": "2f4c2103-90c1-4292-ec3a-dcc964acde4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[0.80956046]\n",
            " [2.0508202 ]]\n",
            "b = 1.5948713189708588\n",
            "dw = [[ 0.17860505]\n",
            " [-0.04840656]]\n",
            "db = -0.08888460336847771\n",
            "Costs = [array(0.15900538)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # compute vector 'A' predicting the probabilities of a cat being present in the picture\n",
        "    A = sigmoid(w.T @ X + b)\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        # convert probabilities A[0, i] to actual predictions p[0, i]\n",
        "        if A[0, i] > 0.5:\n",
        "            Y_prediction[0, i] = 1\n",
        "        else:\n",
        "            Y_prediction[0, i] = 0\n",
        "\n",
        "    return Y_prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "LJZTZNyAuEwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([[0.1124579], [0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrKudyWYuLn8",
        "outputId": "04f313cb-ba43-417b-eeb1-c64615c33ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions = [[1. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "8XrIW2H4uOy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training dataset\n",
        "train_set_x= np.array([[1,1,1],[1,0,1],[1,0,0],[0,1,1],[0,0,0],[0,1,0]]).T\n",
        "train_set_y=np.array([1,1,0,1,0,0])\n",
        "test_set_x= np.array([[0,0,0],[0,0,1],[1,1,0],[1,1,1],[0,1,0],[1,1,0]]).T\n",
        "test_set_y=np.array([0,0,1,1,0,1])"
      ],
      "metadata": {
        "id": "h9s5CVTxxcn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvhBUjC5ydac",
        "outputId": "18994454-a278-4f6e-b97d-4d842456f796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.658669\n",
            "Cost after iteration 200: 0.629929\n",
            "Cost after iteration 300: 0.605060\n",
            "Cost after iteration 400: 0.582895\n",
            "Cost after iteration 500: 0.562704\n",
            "Cost after iteration 600: 0.544026\n",
            "Cost after iteration 700: 0.526563\n",
            "Cost after iteration 800: 0.510118\n",
            "Cost after iteration 900: 0.494556\n",
            "Cost after iteration 1000: 0.479779\n",
            "Cost after iteration 1100: 0.465716\n",
            "Cost after iteration 1200: 0.452309\n",
            "Cost after iteration 1300: 0.439512\n",
            "Cost after iteration 1400: 0.427286\n",
            "Cost after iteration 1500: 0.415596\n",
            "Cost after iteration 1600: 0.404412\n",
            "Cost after iteration 1700: 0.393705\n",
            "Cost after iteration 1800: 0.383450\n",
            "Cost after iteration 1900: 0.373623\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 50.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzcPK29FtNQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}